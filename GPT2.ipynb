{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eNcoDkzjZMuX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "  def __init__(self, d_model, max_len=5000):\n",
        "    super().__init__()\n",
        "\n",
        "    pe=torch.zeros(max_len, d_model)\n",
        "    position=torch.arange(0,max_len).unsqueeze(1)\n",
        "\n",
        "    div_term=torch.exp(torch.arange(0,d_model,2) * (-math.log(10000.0)/d_model))\n",
        "    pe[:, 0::2]=torch.sin(position*div_term)\n",
        "    pe[:, 1::2]= torch.cos(position*div_term)\n",
        "\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "      seq_len=x.size(1)\n",
        "      return x + self.pe[:seq_len]"
      ],
      "metadata": {
        "id": "6CShYtJnCJ5l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(d_model=8, max_len=10)\n",
        "\n",
        "print(\"Position 0 encoding:\")\n",
        "print(pe.pe[0])\n",
        "\n",
        "print(\"\\nPosition 1 encoding:\")\n",
        "print(pe.pe[1])\n",
        "\n",
        "print(\"\\nPosition 2 encoding:\")\n",
        "print(pe.pe[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isMRyCWTZavj",
        "outputId": "3bf405dc-34b5-44b7-cb35-dad5bac01fd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position 0 encoding:\n",
            "tensor([0., 1., 0., 1., 0., 1., 0., 1.])\n",
            "\n",
            "Position 1 encoding:\n",
            "tensor([0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000])\n",
            "\n",
            "Position 2 encoding:\n",
            "tensor([ 0.9093, -0.4161,  0.1987,  0.9801,  0.0200,  0.9998,  0.0020,  1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = torch.nn.Linear(d_model, d_model)\n",
        "        self.W_k = torch.nn.Linear(d_model, d_model)\n",
        "        self.W_v = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        out = torch.matmul(weights, V)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "qp2GTUi_WJ9H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 5, 8)\n",
        "\n",
        "attn = SingleHeadSelfAttention(d_model=8)\n",
        "out = attn(x)\n",
        "\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fydW9ITfupUs",
        "outputId": "ee22feba-46ea-446e-e75f-a2598cbd8acc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = torch.nn.Linear(d_model, d_model)\n",
        "        self.W_k = torch.nn.Linear(d_model, d_model)\n",
        "        self.W_v = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_o = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, V)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return self.W_o(out)\n"
      ],
      "metadata": {
        "id": "CqZMxSfcuqZj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 5, 8)\n",
        "\n",
        "mha = MultiHeadSelfAttention(d_model=8, num_heads=2)\n",
        "out = mha(x)\n",
        "\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-3PXHhIwLTX",
        "outputId": "8288dac9-d01b-4c70-dabe-e993a34d3f6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 8])\n"
          ]
        }
      ]
    }
  ]
}